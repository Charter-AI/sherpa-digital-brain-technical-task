{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OShHgRCZAyTa"
      },
      "source": [
        "# Sherpa Digital Brain Technical Task\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "*Git instructions*\n",
        "\n",
        "## Task\n",
        "\n",
        "*Task instructions*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHVZzkn2AyTc"
      },
      "outputs": [],
      "source": [
        "# Git magic to copy additional folders from github repo\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/Charter-AI/sherpa-digital-brain-technical-task.git\"\n",
        "REPO_NAME = \"sherpa-digital-brain-technical-task\"\n",
        "REPO_PATH = f\"/content/{REPO_NAME}\"\n",
        "\n",
        "\n",
        "# Clone only if the repo folder doesn't already exist\n",
        "if not os.path.exists(REPO_PATH):\n",
        "    print(f\"Cloning {REPO_URL} into {REPO_PATH}\")\n",
        "    os.chdir('/content')\n",
        "    !git clone $REPO_URL\n",
        "    os.chdir(REPO_PATH)\n",
        "else:\n",
        "    print(f\"Repo '{REPO_NAME}' already exists at {REPO_PATH}. \\nSkipping clone and pulling instead...\")\n",
        "    os.chdir(REPO_PATH)\n",
        "    !git pull\n",
        "\n",
        "# Confirm it worked\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"Files in current directory:\", os.listdir('.'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Used later to make all references line up\n",
        "project_root = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZAJ_fXIfv6E"
      },
      "outputs": [],
      "source": [
        "# Download byaldi embeddings and save to /content/sherpa-digital-brain-technical-task/.byaldi/default_index\n",
        "\n",
        "%pip install requests\n",
        "%pip install tqdm\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Public URL to the ZIP file in Azure Blob Storage\n",
        "ZIP_URL = \"https://sherpapublicdata.blob.core.windows.net/byaldi-embeddings/MBB_AI_byaldi_embeddings.zip\"\n",
        "\n",
        "# Where the extracted files should go\n",
        "EXTRACTED_FOLDER = \"/content/sherpa-digital-brain-technical-task/.byaldi/default_index\"\n",
        "ZIP_FILENAME = \"MBB_AI_byaldi_embeddings.zip\"\n",
        "\n",
        "# Check if folder is already extracted\n",
        "if not os.path.exists(EXTRACTED_FOLDER):\n",
        "    # Download ZIP file\n",
        "    print(f\"Downloading {ZIP_FILENAME}...\")\n",
        "    response = requests.get(ZIP_URL, stream=True)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Failed to download ZIP file: {response.status_code}\")\n",
        "\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    with open(ZIP_FILENAME, \"wb\") as f, tqdm(desc=ZIP_FILENAME, total=total_size, unit='B', unit_scale=True, unit_divisor=1024) as bar:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "                bar.update(len(chunk))\n",
        "\n",
        "    # Extract ZIP\n",
        "    print(f\"Extracting {ZIP_FILENAME}...\")\n",
        "    with zipfile.ZipFile(ZIP_FILENAME, 'r') as zip_ref:\n",
        "        zip_ref.extractall(EXTRACTED_FOLDER)\n",
        "\n",
        "    # Cleanup if needed\n",
        "    os.remove(ZIP_FILENAME)\n",
        "    print(f\"Extraction complete. Files are in '{EXTRACTED_FOLDER}'\")\n",
        "else:\n",
        "    print(f\"Folder '{EXTRACTED_FOLDER}' already exists. Skipping download and extraction.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzXTNVvaAyTd"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# Takes ~2min\n",
        "\n",
        "%pip install --upgrade pip\n",
        "\n",
        "%pip install numpy\n",
        "%pip install torch\n",
        "%pip install openai\n",
        "%pip install python-dotenv\n",
        "\n",
        "!apt-get install -y poppler-utils\n",
        "\n",
        "%pip install byaldi\n",
        "%pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "di_xwA9QAyTf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sebastian/Coding/sherpa-digital-brain-technical-task/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import dependencies\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# from openai import AzureOpenAI\n",
        "from byaldi import RAGMultiModalModel\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as IPythonImage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJZvVb-fWlf9"
      },
      "outputs": [],
      "source": [
        "# LLM functions (OpenAI GPT)\n",
        "load_dotenv()\n",
        "\n",
        "# Access environment variables\n",
        "AZURE_OPENAI_MODEL = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
        "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "AZURE_OPENAI_VERSION = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
        "\n",
        "openai_client = AzureOpenAI(\n",
        "    api_key=AZURE_OPENAI_API_KEY,\n",
        "    api_version=AZURE_OPENAI_VERSION,\n",
        "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
        ")\n",
        "\n",
        "\n",
        "def query_llm(\n",
        "        query: str,\n",
        "        system_prompt: str = \"You are a helpful assistant.\",\n",
        "        b64_images: list[str] | None = None,\n",
        "        raw_output: bool = False\n",
        ") -> str:\n",
        "    user_content = [{\"type\": \"text\", \"text\": query}]\n",
        "\n",
        "    # Send b64 images to LLM if provided\n",
        "    if b64_images is not None:\n",
        "        user_content.extend([\n",
        "            {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\n",
        "                    \"url\": f\"data:image/jpeg;base64,{b64_image}\"\n",
        "                }\n",
        "            } for b64_image in b64_images\n",
        "        ])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_content}\n",
        "    ]\n",
        "\n",
        "    raw_response = openai_client.chat.completions.create(\n",
        "        model=AZURE_OPENAI_MODEL,\n",
        "        messages=messages,\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    if raw_output:\n",
        "        return raw_response\n",
        "    else:\n",
        "        return raw_response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LRi64kL4AyTg"
      },
      "outputs": [],
      "source": [
        "# Byaldi functions\n",
        "default_index = \"default_index\"\n",
        "\n",
        "def initialise_rag_model(index_name: str = default_index, device: str | None = None) -> RAGMultiModalModel:\n",
        "    \"\"\"\n",
        "    Initialises the RAG model with index stored at `.byaldi/<index_name>`.\n",
        "    If index does not exist, index will be created.\n",
        "    If index does exist, it will be loaded.\n",
        "    \"\"\"\n",
        "    rag_model = None\n",
        "\n",
        "    # Use CUDA if available, otherwise use CPU\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    else:\n",
        "        device = \"cuda\" if device == \"gpu\" else \"cpu\"\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # If .byaldi/<index_name> folder exists, load index from there\n",
        "    # Otherwise, load generic model with vidore/colqwen2-v1.0 vision model\n",
        "\n",
        "    index_path = project_root + \"/.byaldi/\" + index_name\n",
        "    if os.path.exists(index_path):\n",
        "        print(f\"Loading index from {index_path}\")\n",
        "        rag_model = RAGMultiModalModel.from_index(index_path, device=device)\n",
        "    else:\n",
        "        print(f\"No index found at {index_path}. Loading generic vidore/colqwen2-v1.0 model.\")\n",
        "        rag_model = RAGMultiModalModel.from_pretrained(\n",
        "            \"vidore/colqwen2-v1.0\",\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    return rag_model\n",
        "\n",
        "def index_documents(rag_model: RAGMultiModalModel, index_name: str = default_index, docs_folder_name: str = \"documents\"):\n",
        "    \"\"\"\n",
        "    Indexes documents for RAG model in `docs_folder_name` and stores index at `.byaldi/<index_name>`.\n",
        "    If index already exists, it will be overwritten.\n",
        "    \"\"\"\n",
        "    project_root + \"/.byaldi/\" + index_name\n",
        "    docs_path = project_root + \"/\" + docs_folder_name\n",
        "\n",
        "    # Check if documents folder exists\n",
        "    if not os.path.exists(docs_path):\n",
        "        raise FileNotFoundError(f\"Documents folder {docs_path} does not exist\")\n",
        "\n",
        "    rag_model.index(\n",
        "        input_path=docs_path,\n",
        "        index_name=index_name,\n",
        "        store_collection_with_index=True,    # Store base64 encodings of documents with index\n",
        "        max_image_height=2048,\n",
        "        max_image_width=2048,\n",
        "        overwrite=True\n",
        "    )\n",
        "\n",
        "def query_rag_model(rag_model: RAGMultiModalModel, query: str, k: int = 5):\n",
        "    \"\"\"\n",
        "    Queries the RAG model with `query` and returns `k` most relevant documents.\n",
        "\n",
        "    Returns a list of dictionaries, each containing the following keys:\n",
        "    - `doc_id`: ID of document\n",
        "    - `page_num`: Page number of image within document\n",
        "    - `score`: Score of document\n",
        "    - `metadata`: Metadata attached to image\n",
        "    - `base64`: Base64 encoded image\n",
        "    \"\"\"\n",
        "    results = rag_model.search(\n",
        "        query=query,\n",
        "        k=k\n",
        "    )\n",
        "\n",
        "    results_dict = [x.__dict__ for x in results]\n",
        "    return results_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tOj6p7oACap5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "No index found at /home/sebastian/Coding/sherpa-digital-brain-technical-task/.byaldi/default_index. Loading generic vidore/colqwen2-v1.0 model.\n",
            "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 35394.97it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.59s/it]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexing file: /home/sebastian/Coding/sherpa-digital-brain-technical-task/documents/bain_report_luxury_and_technology_artificial_intelligence_the_quiet_revolution.pdf.pdf\n",
            "Added page 1 of document 0 to index.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Index all 3 files in /documents folder\n",
        "# Takes ~mins on CPU\n",
        "rag_model = initialise_rag_model(device=\"cpu\")\n",
        "index_documents(rag_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7GFilCkEPkd"
      },
      "outputs": [],
      "source": [
        "# Query RAG model and display retrieved images\n",
        "example_queries = [\n",
        "    \"Who at Bain and Company should be assigned to a project on integrating AI into the luxury industry?\",\n",
        "    \"What frameworks can be used to conceptualise AI integration into modern businesses?\",\n",
        "    \"Summarise that McKinsey report on generative AI\",\n",
        "]\n",
        "\n",
        "rag_model = initialise_rag_model()\n",
        "\n",
        "for query in example_queries:\n",
        "    results = query_rag_model(rag_model, query)\n",
        "    images = [result[\"base64\"] for result in results]\n",
        "\n",
        "    print(f\"Query: {query}\")\n",
        "    for result in results:\n",
        "        print(f\"Retreived page {result['page_num']} from document {result['doc_id']} with score {result['score']}\")\n",
        "\n",
        "# for result in results:\n",
        "#     display(IPythonImage(data=base64.b64decode(result[\"base64\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrF9F0wzYayT"
      },
      "outputs": [],
      "source": [
        "llm_response = query_llm(example_query, b64_images=images)\n",
        "print(llm_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
